{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing - check if Standard Sxcaler will work well\n",
    "# Alternatively, divide by 200?\n",
    "# source: https://medium.com/analytics-vidhya/introduction-to-neural-networks-for-finance-6abd5675e497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7106, 56)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camel_data = pd.read_csv('data/camel_data_timeseries.csv', index_col = 0)\n",
    "camel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6898, 56)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camel_data.dropna(inplace=True)\n",
    "camel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6892, 56)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#camel_data.replace(np.inf, '', inplace=True)\n",
    "#df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "camel_set = camel_data[~camel_data.isin([np.nan, 'NaN', np.inf, -np.inf]).any(1)]\n",
    "\n",
    "#camel_set = camel_data.dropna()\n",
    "camel_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#camel_set.columns\n",
    "len(camel_set.loc[camel_set['Target'] ==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = camel_set['Target']\n",
    "X = camel_set.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=1)\n",
    "log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5483,    9],\n",
       "       [  10,   11]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_y_train_pred = cross_val_predict(log_reg, X_train_std, y_train, cv=3)\n",
    "confusion_matrix(y_train, log_y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5365853658536585"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, log_y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 15:37:38.592757 4612675008 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide as model inputs either a single array or a list of arrays. You passed: x=             EQTA      EQTL     LLRTA     LLRGL     OEXTA    INCEMP       ROA  \\\n3602889  0.103090  0.144379  0.011877  0.016633  0.015670   31.0952  0.004323   \n757612   0.094668  0.142027  0.010604  0.015908  0.018137   21.7458  0.004717   \n3341393  0.091561  0.112903  0.012986  0.016012  0.020494   18.3182  0.003754   \n324863   0.171461  0.260938  0.011817  0.017984  0.016920   33.4372  0.008096   \n609010   0.092540  0.107180  0.009723  0.011261  0.024456   21.7606  0.007063   \n3151417  0.097800  0.153933  0.028652  0.045096  0.019936   19.0833  0.003496   \n662640   0.108648  0.186248  0.006489  0.011124  0.017136   29.4568  0.007654   \n929044   0.113459  0.179765  0.012400  0.019647  0.026510  -5.73077 -0.001833   \n852973   0.150214  0.201185  0.011996  0.016067  0.017908   78.3077  0.017944   \n44433    0.089974  0.122943  0.020264  0.027689  0.040048  -70.6932 -0.023691   \n330239   0.126168  0.298281  0.005793  0.013696  0.018675   41.8182  0.013392   \n325141   0.201310  0.418300  0.006883  0.014303  0.029909   15.5397  0.005090   \n490544   0.098683  0.149011  0.007712  0.011645  0.048727     -65.5 -0.026820   \n528102   0.133122  0.235212  0.004620  0.008164  0.020367   31.5319  0.007781   \n137447   0.112484  0.197790  0.004048  0.007118  0.023186   21.3111  0.007487   \n1002355  0.075966  0.140480  0.009670  0.017882  0.028081      -5.1 -0.001755   \n191449   0.122825  0.322843  0.002329  0.006123  0.019683   17.2727  0.004426   \n2577739  0.115567  0.182773  0.012640  0.019990  0.019544   27.4348  0.004917   \n102539   0.104854  0.259828  0.004441  0.011005  0.022124      5.05  0.002086   \n2114282  0.152128  0.199075  0.024608  0.032202  0.028849  -6.82058 -0.002196   \n410560   0.102282  0.180049  0.003930  0.006918  0.019705   37.8125  0.009825   \n738143   0.099938  0.134120  0.007218  0.009687  0.022059   34.8987  0.009632   \n848752   0.121918  0.201750  0.008765  0.014505  0.015282    52.303  0.008920   \n43351    0.106105  0.310857  0.004443  0.013017  0.029059  -35.8333 -0.019106   \n179335   0.114787  0.175868  0.008741  0.013393  0.020172   34.5814  0.007999   \n323035   0.099122  0.143958  0.017601  0.025562  0.028216  -62.6154 -0.017399   \n230254   0.129546  0.242799  0.007706  0.014442  0.021494   4.33333  0.001506   \n2975625  0.020170  0.029097  0.019904  0.028713  0.031462  -45.4667 -0.012109   \n501655   0.120592  0.215515  0.007553  0.013498  0.018499   24.5652  0.008835   \n290230   0.123956  0.209469  0.008811  0.014890  0.018048   27.1579  0.007930   \n...           ...       ...       ...       ...       ...       ...       ...   \n106377   0.115491  0.155515  0.011970  0.016118  0.019542   15.7039  0.003068   \n859150   0.100730  0.139536  0.014409  0.019961  0.019562   43.5714  0.013908   \n107226   0.202949  0.402896  0.005670  0.011256  0.017409   32.1667  0.007706   \n3720608  0.216636  0.324789  0.008441  0.012656  0.021065     16.64  0.002636   \n3715444  0.153870  0.265904  0.010176  0.017585  0.019291  -58.5192 -0.006810   \n964157   0.105475  0.174420  0.009426  0.015587  0.038412   10.3861  0.006636   \n522959   0.104001  0.153123  0.007946  0.011699  0.022551   62.7797  0.018045   \n685658   0.093346  0.120959  0.009340  0.012103  0.031481   30.9901  0.011952   \n3816190  0.104974  0.169622  0.008640  0.013961  0.027284  -51.9615 -0.015238   \n392778   0.167873  0.497653  0.005187  0.015377  0.018007   13.9853  0.003060   \n813059   0.097774  0.185313  0.005985  0.011344  0.022616      12.6  0.002258   \n1229424  0.075410  0.103588  0.012674  0.017410  0.033464  0.833333  0.000275   \n1188987  0.083659  0.166071  0.008487  0.016847  0.024968   21.1429  0.006285   \n411651   0.093624  0.142230  0.007496  0.011388  0.019681        38  0.010323   \n587677   0.173186  0.282256  0.004933  0.008040  0.012838   15.6863  0.002032   \n1000641  0.103654  0.210742  0.015704  0.031929  0.025343   13.4045  0.004929   \n3282021  0.092243  0.118185  0.017233  0.022080  0.015909      37.3  0.005611   \n25357    0.087734  0.221243  0.005684  0.014333  0.022690   10.0612  0.003711   \n2730477  0.089601  0.108924  0.019398  0.023582  0.020022     -25.8 -0.004470   \n654047   0.171666  0.273919  0.012513  0.019967  0.020108   26.3571  0.006369   \n941541   0.089918  0.133194  0.013784  0.020418  0.022550   3.14286  0.000887   \n815754   0.080322  0.120248  0.010879  0.016287  0.028625   15.0789  0.005837   \n979759   0.089137  0.277061  0.004623  0.014369  0.022445   19.3704  0.004865   \n172859   0.105476  0.142972  0.007791  0.010560  0.017602   47.9444  0.009688   \n813853   0.095959  0.130506  0.031242  0.042490  0.026120  -39.0615 -0.010794   \n201049   0.114724  0.183763  0.008274  0.013253  0.023409   25.5882  0.007315   \n659257   0.088320  0.129736  0.016955  0.024905  0.020988   12.4565  0.002574   \n819855   0.093316  0.118152  0.007034  0.008907  0.023710   22.0714  0.006158   \n2721765  0.098232  0.123307  0.030111  0.037797  0.013672   -262.16 -0.019543   \n721752   0.149228  0.294718  0.012952  0.025580  0.021944   9.52941  0.003099   \n\n              ROE      TDTL      TDTA  ...   EQTL-4Q  LLRTA-4Q  LLRGL-4Q  \\\n3602889  0.041934  1.241417  0.886397  ...  0.134116  0.011124  0.014795   \n757612   0.049829  1.285769  0.857028  ...  0.141138  0.010124  0.014670   \n3341393  0.041001  1.109273  0.899589  ...  0.109558  0.011268  0.013574   \n324863   0.047219  1.247815  0.819933  ...  0.256310  0.011637  0.018342   \n609010   0.076319  1.043430  0.900909  ...  0.101123  0.009407  0.010835   \n3151417  0.035742  1.373745  0.872800  ...  0.152801  0.024054  0.037007   \n662640   0.070446  1.389480  0.810553  ...  0.174733  0.007035  0.011518   \n929044  -0.016155  1.282754  0.809618  ...  0.151211  0.011050  0.016505   \n852973   0.119453  1.131780  0.845041  ...  0.191034  0.010234  0.013404   \n44433   -0.263312  0.988651  0.723529  ...  0.119802  0.023710  0.032031   \n330239   0.106141  1.852739  0.783676  ...  0.249984  0.006337  0.013679   \n325141   0.025283  1.651334  0.794718  ...  0.392594  0.005478  0.010535   \n490544  -0.271784  1.351814  0.895243  ...  0.153898  0.009696  0.014275   \n528102   0.058452  1.515205  0.857553  ...  0.214840  0.004370  0.007478   \n137447   0.066558  1.524802  0.867163  ...  0.200149  0.004199  0.007687   \n1002355 -0.023103  1.704817  0.921900  ...  0.118505  0.016471  0.028869   \n191449   0.036033  2.285557  0.869535  ...  0.282872  0.003840  0.009480   \n2577739  0.042549  1.262019  0.797972  ...  0.182145  0.010555  0.016815   \n102539   0.019898  2.204648  0.889692  ...  0.245245  0.004409  0.010248   \n2114282 -0.014437  0.910195  0.695550  ...  0.152440  0.031910  0.039015   \n410560   0.096062  1.404134  0.797660  ...  0.170950  0.004227  0.006817   \n738143   0.096378  1.079846  0.804634  ...  0.144787  0.006509  0.010443   \n848752   0.073167  1.446048  0.873849  ...  0.188952  0.013265  0.019847   \n43351   -0.180067  2.614423  0.892384  ...  1.140924  0.001174  0.012401   \n179335   0.069685  1.337034  0.872663  ...  0.160615  0.009002  0.013075   \n323035  -0.175527  1.290706  0.888713  ...  0.168263  0.008327  0.014486   \n230254   0.011628  1.615305  0.861850  ...  0.226174  0.008827  0.014839   \n2975625 -0.600352  1.338384  0.927754  ...  0.036091  0.020346  0.026383   \n501655   0.073262  1.497457  0.837907  ...  0.124730  0.008891  0.010447   \n290230   0.063975  1.458551  0.863119  ...  0.203610  0.008582  0.013932   \n...           ...       ...       ...  ...       ...       ...       ...   \n106377   0.026566  0.964458  0.716242  ...  0.142343  0.012407  0.016509   \n859150   0.138072  1.241678  0.896352  ...  0.121575  0.013577  0.017909   \n107226   0.037972  1.580114  0.795946  ...  0.399340  0.005624  0.011313   \n3720608  0.012169  1.131316  0.754593  ...  0.432366  0.007707  0.013025   \n3715444 -0.044259  1.422888  0.823381  ...  0.376697  0.011836  0.017937   \n964157   0.062916  1.449059  0.876268  ...  0.155126  0.009551  0.014171   \n522959   0.173506  1.314768  0.892984  ...  0.132995  0.007071  0.009619   \n685658   0.128043  1.168461  0.901728  ...  0.114324  0.008436  0.010443   \n3816190 -0.145160  1.417923  0.877510  ...  0.434825  0.006592  0.016319   \n392778   0.018229  2.376257  0.801583  ...  0.507635  0.003679  0.010653   \n813059   0.023094  1.642348  0.866528  ...  0.181502  0.005903  0.011323   \n1229424  0.003646  1.204657  0.876961  ...  0.098415  0.015155  0.019999   \n1188987  0.075121  1.812651  0.913132  ...  0.094599  0.007841  0.014227   \n411651   0.110263  1.336064  0.879471  ...  0.134915  0.009179  0.012300   \n587677   0.011734  1.346591  0.826237  ...  0.278366  0.003487  0.005337   \n1000641  0.047553  1.803807  0.887207  ...  0.205194  0.013754  0.027733   \n3282021  0.060833  1.094708  0.854418  ...  0.120272  0.011887  0.015496   \n25357    0.042303  1.607024  0.637266  ...  0.216325  0.004856  0.012244   \n2730477 -0.049884  1.091372  0.897762  ...  0.093360  0.011482  0.014096   \n654047   0.037100  1.309199  0.820481  ...  0.252687  0.013710  0.020019   \n941541   0.009861  1.268030  0.856035  ...  0.121282  0.006791  0.009497   \n815754   0.072670  1.369375  0.914707  ...  0.111406  0.010095  0.014726   \n979759   0.054576  2.796172  0.899590  ...  0.288980  0.004700  0.014017   \n172859   0.091848  1.180420  0.870838  ...  0.131345  0.007687  0.010611   \n813853  -0.112484  1.105795  0.813073  ...  0.137363  0.016810  0.022633   \n201049   0.063764  1.315349  0.821179  ...  0.170824  0.009062  0.013390   \n659257   0.029142  1.106535  0.753293  ...  0.119676  0.013727  0.019579   \n819855   0.065986  1.131635  0.893767  ...  0.116514  0.005499  0.006853   \n2721765 -0.198950  0.973118  0.775230  ...  0.135236  0.027052  0.031149   \n721752   0.020769  1.590531  0.805353  ...  0.272040  0.014499  0.025533   \n\n         OEXTA-4Q  INCEMP-4Q    ROA-4Q    ROE-4Q   TDTL-4Q   TDTA-4Q   TATA-4Q  \n3602889  0.020256    8.63158  0.001215  0.012045  1.177830  0.885625  0.118922  \n757612   0.025860    19.5263  0.004402  0.045189  1.231666  0.850008  0.136764  \n3341393  0.025765      19.85  0.003827  0.042077  1.082549  0.898609  0.029042  \n324863   0.020616    39.5944  0.009364  0.057584  1.310743  0.831595  0.114862  \n609010   0.034064    21.0822  0.006964  0.079318  1.044268  0.906663  0.013665  \n3151417  0.030373    -123.75 -0.023877 -0.240408  1.325962  0.861865  0.076343  \n662640   0.024189    30.3412  0.008655  0.081089  1.330600  0.812755  0.268870  \n929044   0.031658    11.0833  0.003124  0.030855  1.230298  0.823661  0.217085  \n852973   0.024255    103.492  0.023874  0.163675  1.113374  0.850109  0.188822  \n44433    0.062214   -154.382 -0.047385 -0.534339  0.988679  0.731832  0.195691  \n330239   0.025644    48.0795  0.015571  0.134446  1.711839  0.793096  0.448697  \n325141   0.039359    32.7778  0.010742  0.052626  1.525093  0.792960  0.002159  \n490544   0.051695   -41.2857 -0.016582 -0.158617  1.311090  0.890585  0.135234  \n528102   0.028352    27.1739  0.006622  0.052738  1.478862  0.864310  0.100700  \n137447   0.031955    23.7931  0.008239  0.075358  1.611915  0.880557  0.312619  \n1002355  0.040982    -36.913 -0.013846 -0.204776  1.629709  0.929842  0.232669  \n191449   0.028229    18.9091  0.004930  0.043029  2.175727  0.881254  0.473395  \n2577739  0.026678     22.931  0.005207  0.045542  1.312136  0.823663  0.276533  \n102539   0.028236      -2.35 -0.000964 -0.009135  2.066924  0.889149  0.000000  \n2114282  0.039955   -29.9495 -0.009569 -0.076752  0.823919  0.673889  0.108766  \n410560   0.029029      45.75  0.013946  0.131582  1.435894  0.890227  0.145566  \n738143   0.022790      35.15  0.009715  0.107657  1.370260  0.853992  0.184027  \n848752   0.020448    95.9412  0.017370  0.137550  1.297049  0.866863  0.219136  \n43351    0.032761     -56.25 -0.024010 -0.222332  9.392334  0.889019  0.190268  \n179335   0.028708    42.6667  0.009927  0.089771  1.279269  0.880809  0.211940  \n323035   0.028946    5.76692  0.001472  0.015219  1.368052  0.786392  0.026184  \n230254   0.032537    13.4783  0.004718  0.035068  1.438762  0.855791  0.000000  \n2975625  0.053945   -195.057 -0.058411 -2.098678  1.140237  0.879311  0.071245  \n501655   0.032881    48.2308  0.016019  0.150903  0.936810  0.797297  0.067270  \n290230   0.026662         28  0.008665  0.069087  1.398186  0.861228  0.195910  \n...           ...        ...       ...       ...       ...       ...       ...  \n106377   0.026074   -89.0647 -0.016687 -0.155979  0.917026  0.689205  0.142440  \n859150   0.027968     42.125  0.014434  0.156599  1.137845  0.862643  0.041246  \n107226   0.023595    43.6111  0.010388  0.052326  1.610270  0.800532  0.000000  \n3720608  0.032769     -31.25 -0.005780 -0.022594  1.215636  0.719263  0.243836  \n3715444  0.040310   -172.923 -0.032194 -0.129514  1.084021  0.715322  0.135692  \n964157   0.056431    11.7653  0.007461  0.071362  1.287344  0.867603  0.130863  \n522959   0.031549      78.35  0.023342  0.238775  1.225587  0.900858  0.153567  \n685658   0.043273    40.0882  0.016466  0.178294  1.117654  0.902863  0.111050  \n3816190  0.029055    -74.125 -0.029318 -0.166917  2.024479  0.817779  0.000000  \n392778   0.023890    14.8438  0.003279  0.018701  2.280504  0.787624  0.506597  \n813059   0.031114   -37.1667 -0.007930 -0.083803  1.676079  0.873800  0.052059  \n1229424  0.047777   -17.7395 -0.006201 -0.083146  1.154411  0.874775  0.096335  \n1188987  0.030573      26.52  0.007841  0.150391  1.716300  0.945973  0.396222  \n411651   0.025608    41.2143  0.011353  0.112761  1.171274  0.874074  0.068817  \n587677   0.017219    25.5294  0.003517  0.019338  1.250976  0.817270  0.061007  \n1000641  0.032609    8.45455  0.003039  0.029865  1.796602  0.890980  0.185594  \n3282021  0.020606       27.7  0.004399  0.047681  1.106670  0.848958  0.057261  \n25357    0.029808    29.2308  0.011444  0.133380  1.454670  0.576979  0.068666  \n2730477  0.026330    6.27027  0.001150  0.015125  1.085344  0.884042  0.049670  \n654047   0.028155    20.9333  0.005763  0.033302  1.174729  0.804548  0.143382  \n941541   0.027824       30.5  0.008529  0.098342  1.203681  0.860741  0.168521  \n815754   0.039001      16.75  0.006853  0.089728  1.326535  0.909417  0.203965  \n979759   0.044708   -8.18519 -0.002473 -0.025523  2.661494  0.892476  0.498092  \n172859   0.024757    65.1765  0.012452  0.130861  1.173151  0.849883  0.074384  \n813853   0.033182   -12.6364 -0.003410 -0.033427  1.092262  0.811231  0.100316  \n201049   0.031439    22.2703  0.007011  0.060646  1.193467  0.807688  0.218399  \n659257   0.026610    28.7316  0.005980  0.071269  1.081023  0.757917  0.108124  \n819855   0.032366    26.8088  0.007664  0.081977  1.113711  0.893599  0.036477  \n2721765  0.016337   -43.3478 -0.002965 -0.025246  0.898533  0.780353  0.012696  \n721752   0.031503   0.470588  0.000168  0.001090  1.430721  0.812475  0.327785  \n\n[5518 rows x 55 columns]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bff0a5bcbe8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#(https://medium.com/analytics-vidhya/introduction-to-neural-networks-for-finance-6abd5675e497)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2543\u001b[0m             not tensor_util.is_tensor(x_input)):\n\u001b[1;32m   2544\u001b[0m           raise ValueError('Please provide as model inputs either a single '\n\u001b[0;32m-> 2545\u001b[0;31m                            'array or a list of arrays. You passed: x=' + str(x))\n\u001b[0m\u001b[1;32m   2546\u001b[0m         \u001b[0mall_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Please provide as model inputs either a single array or a list of arrays. You passed: x=             EQTA      EQTL     LLRTA     LLRGL     OEXTA    INCEMP       ROA  \\\n3602889  0.103090  0.144379  0.011877  0.016633  0.015670   31.0952  0.004323   \n757612   0.094668  0.142027  0.010604  0.015908  0.018137   21.7458  0.004717   \n3341393  0.091561  0.112903  0.012986  0.016012  0.020494   18.3182  0.003754   \n324863   0.171461  0.260938  0.011817  0.017984  0.016920   33.4372  0.008096   \n609010   0.092540  0.107180  0.009723  0.011261  0.024456   21.7606  0.007063   \n3151417  0.097800  0.153933  0.028652  0.045096  0.019936   19.0833  0.003496   \n662640   0.108648  0.186248  0.006489  0.011124  0.017136   29.4568  0.007654   \n929044   0.113459  0.179765  0.012400  0.019647  0.026510  -5.73077 -0.001833   \n852973   0.150214  0.201185  0.011996  0.016067  0.017908   78.3077  0.017944   \n44433    0.089974  0.122943  0.020264  0.027689  0.040048  -70.6932 -0.023691   \n330239   0.126168  0.298281  0.005793  0.013696  0.018675   41.8182  0.013392   \n325141   0.201310  0.418300  0.006883  0.014303  0.029909   15.5397  0.005090   \n490544   0.098683  0.149011  0.007712  0.011645  0.048727     -65.5 -0.026820   \n528102   0.133122  0.235212  0.004620  0.008164  0.020367   31.5319  0.007781   \n137447   0.112484  0.197790  0.004048  0.007118  0.023186   21.3111  0.007487   \n1002355  0.075966  0.140480  0.009670  0.017882  0.028081      -5.1 -0.001755   \n191449   0.122825  0.322843  0.002329  0.006123  0.019683   17.2727  0.004426   \n2577739  0.115567  0.182773  0.012640  0.019990  0.019544   27.4348  0.004917   \n102539   0.104854  0.259828  0.004441  0.011005  0.022124      5.05  0.002086   \n2114282  0.152128  0.199075  0.024608  0.032202  0.028849  -6.82058 -0.002196   \n410560   0.102282  0.180049  0.003930  0.006918  0.019705   37.8125  0.009825   \n738143   0.099938  0.134120  0.007218  0.009687  0.022059   34.8987  0.009632   \n848752   0.121918  0.201750  0.008765  0.014505  0.015282    52.303  0.008920   \n43351    0.106105  0.310857  0.004443  0.013017  0.029059  -35.8333 -0.019106   \n179335   0.114787  0.175868  0.008741  0.013393  0.020172   34.5814  0.007999   \n323035   0.099122  0.143958  0.017601  0.025562  0.028216  -62.6154 -0.017399   \n230254   0.129546  0.242799  0.007706  0.014442  0.021494   4.33333  0.001506   \n2975625  0.020170  0.029097  0.019904  0.028713  0.031462  -45.4667 -0.012109   \n501655   0.120592  0.215515  0.007553  0.013498  0.018499   24.5652  0.008835   \n290230   0.123956  0.209469  0.008811  0.014890  0.018048   27.1579  0.007930   \n...           ...       ...       ...       ...       ...       ...       ...   \n106377   0.115491  0.155515  0.011970  0.016118  0.019542   15.7039  0.003068   \n859150   0.100730  0.139536  0.014409  0.019961  0.019562   43.5714  0.013908   \n107226   0.202949  0.402896  0.005670  0.011256  0.017409   32.1667  0.007706   \n3720608  0.216636  0.324789  0.008441  0.012656  0.021065     16.64  0.002636   \n3715444  0.153870  0.265904  0.010176  0.017585  0.019291  -58.5192 -0.006810   \n964157   0.105475  0.174420  0.009426  0.015587  0.038412   10.3861  0.006636   \n522959   0.104001  0.153123  0.007946  0.011699  0.022551   62.7797  0.018045   \n685658   0.093346  0.120959  0.009340  0.012103  0.031481   30.9901  0.011952   \n3816190  0.104974  0.169622  0.008640  0.013961  0.027284  -51.9615 -0.015238   \n392778   0.167873  0.497653  0.005187  0.015377  0.018007   13.9853  0.003060   \n813059   0.097774  0.185313  0.005985  0.011344  0.022616      12.6  0.002258   \n1229424  0.075410  0.103588  0.012674  0.017410  0.033464  0.833333  0.000275   \n1188987  0.083659  0.166071  0.008487  0.016847  0.024968   21.1429  0.006285   \n411651   0.093624  0.142230  0.007496  0.011388  0.019681        38  0.010323   \n587677   0.173186  0.282256  0.004933  0.008040  0.012838   15.6863  0.002032   \n1000641  0.103654  0.210742  0.015704  0.031929  0.025343   13.4045  0.004929   \n3282021  0.092243  0.118185  0.017233  0.022080  0.015909      37.3  0.005611   \n25357    0.087734  0.221243  0.005684  0.014333  0.022690   10.0612  0.003711   \n2730477  0.089601  0.108924  0.019398  0.023582  0.020022     -25.8 -0.004470   \n654047   0.171666  0.273919  0.012513  0.019967  0.020108   26.3571  0.006369   \n941541   0.089918  0.133194  0.013784  0.020418  0.022550   3.14286  0.000887   \n815754   0.080322  0.120248  0.010879  0.016287  0.028625   15.0789  0.005837   \n979759   0.089137  0.277061  0.004623  0.014369  0.022445   19.3704  0.004865   \n172859   0.105476  0.142972  0.007791  0.010560  0.017602   47.9444  0.009688   \n813853   0.095959  0.130506  0.031242  0.042490  0.026120  -39.0615 -0.010794   \n201049   0.114724  0.183763  0.008274  0.013253  0.023409   25.5882  0.007315   \n659257   0.088320  0.129736  0.016955  0.024905  0.020988   12.4565  0.002574   \n819855   0.093316  0.118152  0.007034  0.008907  0.023710   22.0714  0.006158   \n2721765  0.098232  0.123307  0.030111  0.037797  0.013672   -262.16 -0.019543   \n721752   0.149228  0.294718  0.012952  0.025580  0.021944   9.52941  0.003099   \n\n              ROE      TDTL      TDTA  ...   EQTL-4Q  LLRTA-4Q  LLRGL-4Q  \\\n3602889  0.041934  1.241417  0.886397  ...  0.134116  0.011124  0.014795   \n757612   0.049829  1.285769  0.857028  ...  0.141138  0.010124  0.014670   \n3341393  0.041001  1.109273  0.899589  ...  0.109558  0.011268  0.013574   \n324863   0.047219  1.247815  0.819933  ...  0.256310  0.011637  0.018342   \n609010   0.076319  1.043430  0.900909  ...  0.101123  0.009407  0.010835   \n3151417  0.035742  1.373745  0.872800  ...  0.152801  0.024054  0.037007   \n662640   0.070446  1.389480  0.810553  ...  0.174733  0.007035  0.011518   \n929044  -0.016155  1.282754  0.809618  ...  0.151211  0.011050  0.016505   \n852973   0.119453  1.131780  0.845041  ...  0.191034  0.010234  0.013404   \n44433   -0.263312  0.988651  0.723529  ...  0.119802  0.023710  0.032031   \n330239   0.106141  1.852739  0.783676  ...  0.249984  0.006337  0.013679   \n325141   0.025283  1.651334  0.794718  ...  0.392594  0.005478  0.010535   \n490544  -0.271784  1.351814  0.895243  ...  0.153898  0.009696  0.014275   \n528102   0.058452  1.515205  0.857553  ...  0.214840  0.004370  0.007478   \n137447   0.066558  1.524802  0.867163  ...  0.200149  0.004199  0.007687   \n1002355 -0.023103  1.704817  0.921900  ...  0.118505  0.016471  0.028869   \n191449   0.036033  2.285557  0.869535  ...  0.282872  0.003840  0.009480   \n2577739  0.042549  1.262019  0.797972  ...  0.182145  0.010555  0.016815   \n102539   0.019898  2.204648  0.889692  ...  0.245245  0.004409  0.010248   \n2114282 -0.014437  0.910195  0.695550  ...  0.152440  0.031910  0.039015   \n410560   0.096062  1.404134  0.797660  ...  0.170950  0.004227  0.006817   \n738143   0.096378  1.079846  0.804634  ...  0.144787  0.006509  0.010443   \n848752   0.073167  1.446048  0.873849  ...  0.188952  0.013265  0.019847   \n43351   -0.180067  2.614423  0.892384  ...  1.140924  0.001174  0.012401   \n179335   0.069685  1.337034  0.872663  ...  0.160615  0.009002  0.013075   \n323035  -0.175527  1.290706  0.888713  ...  0.168263  0.008327  0.014486   \n230254   0.011628  1.615305  0.861850  ...  0.226174  0.008827  0.014839   \n2975625 -0.600352  1.338384  0.927754  ...  0.036091  0.020346  0.026383   \n501655   0.073262  1.497457  0.837907  ...  0.124730  0.008891  0.010447   \n290230   0.063975  1.458551  0.863119  ...  0.203610  0.008582  0.013932   \n...           ...       ...       ...  ...       ...       ...       ...   \n106377   0.026566  0.964458  0.716242  ...  0.142343  0.012407  0.016509   \n859150   0.138072  1.241678  0.896352  ...  0.121575  0.013577  0.017909   \n107226   0.037972  1.580114  0.795946  ...  0.399340  0.005624  0.011313   \n3720608  0.012169  1.131316  0.754593  ...  0.432366  0.007707  0.013025   \n3715444 -0.044259  1.422888  0.823381  ...  0.376697  0.011836  0.017937   \n964157   0.062916  1.449059  0.876268  ...  0.155126  0.009551  0.014171   \n522959   0.173506  1.314768  0.892984  ...  0.132995  0.007071  0.009619   \n685658   0.128043  1.168461  0.901728  ...  0.114324  0.008436  0.010443   \n3816190 -0.145160  1.417923  0.877510  ...  0.434825  0.006592  0.016319   \n392778   0.018229  2.376257  0.801583  ...  0.507635  0.003679  0.010653   \n813059   0.023094  1.642348  0.866528  ...  0.181502  0.005903  0.011323   \n1229424  0.003646  1.204657  0.876961  ...  0.098415  0.015155  0.019999   \n1188987  0.075121  1.812651  0.913132  ...  0.094599  0.007841  0.014227   \n411651   0.110263  1.336064  0.879471  ...  0.134915  0.009179  0.012300   \n587677   0.011734  1.346591  0.826237  ...  0.278366  0.003487  0.005337   \n1000641  0.047553  1.803807  0.887207  ...  0.205194  0.013754  0.027733   \n3282021  0.060833  1.094708  0.854418  ...  0.120272  0.011887  0.015496   \n25357    0.042303  1.607024  0.637266  ...  0.216325  0.004856  0.012244   \n2730477 -0.049884  1.091372  0.897762  ...  0.093360  0.011482  0.014096   \n654047   0.037100  1.309199  0.820481  ...  0.252687  0.013710  0.020019   \n941541   0.009861  1.268030  0.856035  ...  0.121282  0.006791  0.009497   \n815754   0.072670  1.369375  0.914707  ...  0.111406  0.010095  0.014726   \n979759   0.054576  2.796172  0.899590  ...  0.288980  0.004700  0.014017   \n172859   0.091848  1.180420  0.870838  ...  0.131345  0.007687  0.010611   \n813853  -0.112484  1.105795  0.813073  ...  0.137363  0.016810  0.022633   \n201049   0.063764  1.315349  0.821179  ...  0.170824  0.009062  0.013390   \n659257   0.029142  1.106535  0.753293  ...  0.119676  0.013727  0.019579   \n819855   0.065986  1.131635  0.893767  ...  0.116514  0.005499  0.006853   \n2721765 -0.198950  0.973118  0.775230  ...  0.135236  0.027052  0.031149   \n721752   0.020769  1.590531  0.805353  ...  0.272040  0.014499  0.025533   \n\n         OEXTA-4Q  INCEMP-4Q    ROA-4Q    ROE-4Q   TDTL-4Q   TDTA-4Q   TATA-4Q  \n3602889  0.020256    8.63158  0.001215  0.012045  1.177830  0.885625  0.118922  \n757612   0.025860    19.5263  0.004402  0.045189  1.231666  0.850008  0.136764  \n3341393  0.025765      19.85  0.003827  0.042077  1.082549  0.898609  0.029042  \n324863   0.020616    39.5944  0.009364  0.057584  1.310743  0.831595  0.114862  \n609010   0.034064    21.0822  0.006964  0.079318  1.044268  0.906663  0.013665  \n3151417  0.030373    -123.75 -0.023877 -0.240408  1.325962  0.861865  0.076343  \n662640   0.024189    30.3412  0.008655  0.081089  1.330600  0.812755  0.268870  \n929044   0.031658    11.0833  0.003124  0.030855  1.230298  0.823661  0.217085  \n852973   0.024255    103.492  0.023874  0.163675  1.113374  0.850109  0.188822  \n44433    0.062214   -154.382 -0.047385 -0.534339  0.988679  0.731832  0.195691  \n330239   0.025644    48.0795  0.015571  0.134446  1.711839  0.793096  0.448697  \n325141   0.039359    32.7778  0.010742  0.052626  1.525093  0.792960  0.002159  \n490544   0.051695   -41.2857 -0.016582 -0.158617  1.311090  0.890585  0.135234  \n528102   0.028352    27.1739  0.006622  0.052738  1.478862  0.864310  0.100700  \n137447   0.031955    23.7931  0.008239  0.075358  1.611915  0.880557  0.312619  \n1002355  0.040982    -36.913 -0.013846 -0.204776  1.629709  0.929842  0.232669  \n191449   0.028229    18.9091  0.004930  0.043029  2.175727  0.881254  0.473395  \n2577739  0.026678     22.931  0.005207  0.045542  1.312136  0.823663  0.276533  \n102539   0.028236      -2.35 -0.000964 -0.009135  2.066924  0.889149  0.000000  \n2114282  0.039955   -29.9495 -0.009569 -0.076752  0.823919  0.673889  0.108766  \n410560   0.029029      45.75  0.013946  0.131582  1.435894  0.890227  0.145566  \n738143   0.022790      35.15  0.009715  0.107657  1.370260  0.853992  0.184027  \n848752   0.020448    95.9412  0.017370  0.137550  1.297049  0.866863  0.219136  \n43351    0.032761     -56.25 -0.024010 -0.222332  9.392334  0.889019  0.190268  \n179335   0.028708    42.6667  0.009927  0.089771  1.279269  0.880809  0.211940  \n323035   0.028946    5.76692  0.001472  0.015219  1.368052  0.786392  0.026184  \n230254   0.032537    13.4783  0.004718  0.035068  1.438762  0.855791  0.000000  \n2975625  0.053945   -195.057 -0.058411 -2.098678  1.140237  0.879311  0.071245  \n501655   0.032881    48.2308  0.016019  0.150903  0.936810  0.797297  0.067270  \n290230   0.026662         28  0.008665  0.069087  1.398186  0.861228  0.195910  \n...           ...        ...       ...       ...       ...       ...       ...  \n106377   0.026074   -89.0647 -0.016687 -0.155979  0.917026  0.689205  0.142440  \n859150   0.027968     42.125  0.014434  0.156599  1.137845  0.862643  0.041246  \n107226   0.023595    43.6111  0.010388  0.052326  1.610270  0.800532  0.000000  \n3720608  0.032769     -31.25 -0.005780 -0.022594  1.215636  0.719263  0.243836  \n3715444  0.040310   -172.923 -0.032194 -0.129514  1.084021  0.715322  0.135692  \n964157   0.056431    11.7653  0.007461  0.071362  1.287344  0.867603  0.130863  \n522959   0.031549      78.35  0.023342  0.238775  1.225587  0.900858  0.153567  \n685658   0.043273    40.0882  0.016466  0.178294  1.117654  0.902863  0.111050  \n3816190  0.029055    -74.125 -0.029318 -0.166917  2.024479  0.817779  0.000000  \n392778   0.023890    14.8438  0.003279  0.018701  2.280504  0.787624  0.506597  \n813059   0.031114   -37.1667 -0.007930 -0.083803  1.676079  0.873800  0.052059  \n1229424  0.047777   -17.7395 -0.006201 -0.083146  1.154411  0.874775  0.096335  \n1188987  0.030573      26.52  0.007841  0.150391  1.716300  0.945973  0.396222  \n411651   0.025608    41.2143  0.011353  0.112761  1.171274  0.874074  0.068817  \n587677   0.017219    25.5294  0.003517  0.019338  1.250976  0.817270  0.061007  \n1000641  0.032609    8.45455  0.003039  0.029865  1.796602  0.890980  0.185594  \n3282021  0.020606       27.7  0.004399  0.047681  1.106670  0.848958  0.057261  \n25357    0.029808    29.2308  0.011444  0.133380  1.454670  0.576979  0.068666  \n2730477  0.026330    6.27027  0.001150  0.015125  1.085344  0.884042  0.049670  \n654047   0.028155    20.9333  0.005763  0.033302  1.174729  0.804548  0.143382  \n941541   0.027824       30.5  0.008529  0.098342  1.203681  0.860741  0.168521  \n815754   0.039001      16.75  0.006853  0.089728  1.326535  0.909417  0.203965  \n979759   0.044708   -8.18519 -0.002473 -0.025523  2.661494  0.892476  0.498092  \n172859   0.024757    65.1765  0.012452  0.130861  1.173151  0.849883  0.074384  \n813853   0.033182   -12.6364 -0.003410 -0.033427  1.092262  0.811231  0.100316  \n201049   0.031439    22.2703  0.007011  0.060646  1.193467  0.807688  0.218399  \n659257   0.026610    28.7316  0.005980  0.071269  1.081023  0.757917  0.108124  \n819855   0.032366    26.8088  0.007664  0.081977  1.113711  0.893599  0.036477  \n2721765  0.016337   -43.3478 -0.002965 -0.025246  0.898533  0.780353  0.012696  \n721752   0.031503   0.470588  0.000168  0.001090  1.430721  0.812475  0.327785  \n\n[5518 rows x 55 columns]"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) \n",
    "# source: https://medium.com/engineer-quant/multilayer-perceptron-4453615c4337\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Adam optimizer is gaining popularity in the machine learning community because it is \n",
    "#a more efficient algorithm to optimize compared to traditional stochastic gradient descent. \n",
    "#The advantages are best understood by looking at  the advantages of two other extensions of\n",
    "#stochastic gradient descent \n",
    "#(https://medium.com/analytics-vidhya/introduction-to-neural-networks-for-finance-6abd5675e497)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Long Short Term Model (LSTM)\n",
    "# source: https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up the data into training sets\n",
    "def gen_train(self, seq_len):\n",
    "    \"\"\"\n",
    "    Generates training data\n",
    "    :param seq_len: length of window\n",
    "    :return: X_train and Y_train\n",
    "    \"\"\"\n",
    "    for i in range((len(self.stock_train)//seq_len)*seq_len - seq_len - 1):\n",
    "        x = np.array(self.stock_train.iloc[i: i + seq_len, 1])\n",
    "        y = np.array([self.stock_train.iloc[i + seq_len + 1, 1]], np.float64)\n",
    "        self.input_train.append(x)\n",
    "        self.output_train.append(y)\n",
    "    self.X_train = np.array(self.input_train)\n",
    "    self.Y_train = np.array(self.output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_input to have 3 dimensions, but got array with shape (5518, 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2dcd096a5d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    374\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_input to have 3 dimensions, but got array with shape (5518, 55)"
     ]
    }
   ],
   "source": [
    "# Long Short Term Model (LSTM)\n",
    "# source: https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(20, input_shape=(10, 1), return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(20))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "model.fit(X_train, y_train, epochs=20)\n",
    "\n",
    "model.evaluate(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on Backtesting - check how to do it?\n",
    "\n",
    "#Example below. For full blown backtest systems, you will need to consider factors such as \n",
    "#survivorship bias, look ahead bias, market regime change and transaction costs.\n",
    "\n",
    "#Now that we have fitted our models using our training data and evaluated it using our test data, \n",
    "#we can take the assessment a step further by backtesting the model on new data. Backtesting is \n",
    "#essentially running your strategy (or i our case, the prediction algorithm) over data from a period \n",
    "#of time to see the profit and loss, or accuracy of the algorithm. This is done simply by\n",
    "\n",
    "def back_test(strategy, seq_len, ticker, start_date, end_date, dim):\n",
    "    \"\"\"\n",
    "    A simple back test for a given date period\n",
    "    :param strategy: the chosen strategy. Note to have already formed the model, and fitted with training data.\n",
    "    :param seq_len: length of the days used for prediction\n",
    "    :param ticker: company ticker\n",
    "    :param start_date: starting date\n",
    "    :type start_date: \"YYYY-mm-dd\"\n",
    "    :param end_date: ending date\n",
    "    :type end_date: \"YYYY-mm-dd\"\n",
    "    :param dim: dimension required for strategy: 3dim for LSTM and 2dim for MLP\n",
    "    :type dim: tuple\n",
    "    :return: Percentage errors array that gives the errors for every test in the given date range\n",
    "    \"\"\"\n",
    "    data = pdr.get_data_yahoo(ticker, start_date, end_date)\n",
    "    stock_data = data[\"Adj Close\"]\n",
    "    errors = []\n",
    "    for i in range((len(stock_data)//10)*10 - seq_len - 1):\n",
    "        x = np.array(stock_data.iloc[i: i + seq_len, 1]).reshape(dim) / 200\n",
    "        y = np.array(stock_data.iloc[i + seq_len + 1, 1]) / 200\n",
    "        predict = strategy.predict(x)\n",
    "        while predict == 0:\n",
    "            predict = strategy.predict(x)\n",
    "        error = (predict - y) / 100\n",
    "        errors.append(error)\n",
    "        total_error = np.array(errors)\n",
    "    print(f\"Average error = {total_error.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
