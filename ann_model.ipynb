{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing - check if Standard Sxcaler will work well\n",
    "# Alternatively, divide by 200?\n",
    "# source: https://medium.com/analytics-vidhya/introduction-to-neural-networks-for-finance-6abd5675e497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7106, 56)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camel_data = pd.read_csv('data/camel_data_timeseries.csv', index_col = 0)\n",
    "camel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6898, 56)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camel_data.dropna(inplace=True)\n",
    "camel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6892, 56)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#camel_data.replace(np.inf, '', inplace=True)\n",
    "#df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "camel_set = camel_data[~camel_data.isin([np.nan, 'NaN', np.inf, -np.inf]).any(1)]\n",
    "\n",
    "#camel_set = camel_data.dropna()\n",
    "camel_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#camel_set.columns\n",
    "len(camel_set.loc[camel_set['Target'] ==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = camel_set['Target']\n",
    "X = camel_set.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=1)\n",
    "log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5483,    9],\n",
       "       [  10,   11]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_y_train_pred = cross_val_predict(log_reg, X_train_std, y_train, cv=3)\n",
    "confusion_matrix(y_train, log_y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5365853658536585"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, log_y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 16:33:01.165156 4722406848 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide as model inputs either a single array or a list of arrays. You passed: x=             EQTA      EQTL     LLRTA     LLRGL     OEXTA      INCEMP  \\\n487142   0.135469  0.219712  0.004861  0.007883  0.015192   35.090909   \n3593307  0.092253  0.104042  0.036328  0.040970  0.012562   25.666667   \n3134250  0.082356  0.100571  0.008513  0.010396  0.036723    4.153846   \n537355   0.087692  0.207520  0.005452  0.012901  0.024309   10.622222   \n307062   0.127852  0.186926  0.027541  0.040266  0.021548    8.466667   \n3482894  0.128233  0.174938  0.013176  0.017974  0.026977  -11.821429   \n2078290  0.134291  0.227563  0.014798  0.025075  0.025034   16.555455   \n518354   0.148720  0.205192  0.012503  0.017251  0.052498    7.347826   \n904667   0.076193  0.176231  0.004657  0.010771  0.027759   17.541667   \n613334   0.108285  0.182628  0.005635  0.009504  0.016728   26.818182   \n391418   0.179367  0.398005  0.013135  0.029146  0.029504    5.166667   \n349129   0.162108  0.356917  0.007100  0.015632  0.019909   25.704545   \n2140348  0.074561  0.098370  0.013868  0.018296  0.019760   10.393939   \n117458   0.115381  0.168465  0.011076  0.016171  0.016834   24.054054   \n834979   0.055454  0.066917  0.019895  0.024007  0.023497  -61.342105   \n340144   0.099588  0.134056  0.013343  0.017962  0.019614   31.480000   \n2068107  0.139172  0.213593  0.025532  0.039184  0.034852  -39.657895   \n158947   0.126356  0.189798  0.008001  0.012019  0.018810   36.885246   \n122153   0.128872  0.414666  0.006031  0.019407  0.024545    6.941176   \n750154   0.077829  0.108996  0.008580  0.012016  0.018551   43.883333   \n722227   0.043545  0.061762  0.020988  0.029768  0.042536   -9.666667   \n6141     0.090484  0.202586  0.006494  0.014540  0.016302   26.090909   \n444677   0.106678  0.139776  0.001241  0.001626  0.019108   22.333333   \n2936840  0.093773  0.138799  0.017552  0.025981  0.030092   -6.625000   \n937759   0.126390  0.217976  0.006359  0.010967  0.020328   36.515152   \n803658   0.154279  0.260325  0.008834  0.014907  0.018338   42.666667   \n200435   0.093891  0.128525  0.012523  0.017142  0.019415   44.386667   \n719030   0.108328  0.181117  0.005800  0.009698  0.018258   43.100000   \n3548763  0.139389  0.217168  0.009648  0.015032  0.023446   17.000000   \n3067929  0.111422  0.147760  0.049365  0.065465  0.021013  -79.921053   \n...           ...       ...       ...       ...       ...         ...   \n999935   0.108819  0.165464  0.019108  0.029055  0.026945    0.466019   \n412845   0.092181  0.178049  0.005764  0.011134  0.015931   28.635135   \n3623969  0.135794  0.244630  0.015847  0.028548  0.026008   34.629630   \n794345   0.146918  0.418587  0.005396  0.015374  0.008319   51.000000   \n895831   0.039422  0.051168  0.037432  0.048585  0.028732 -130.747253   \n173342   0.111518  0.290531  0.009037  0.023542  0.016247   60.312500   \n805876   0.132621  0.172537  0.005226  0.006799  0.021379   14.341463   \n1225798  0.151043  0.161415  0.016558  0.017695  0.018774   67.181818   \n477303   0.088201  0.106185  0.006826  0.008218  0.023050   13.271930   \n1669     0.182143  0.539039  0.008249  0.024414  0.014856   47.122807   \n3602889  0.103090  0.144379  0.011877  0.016633  0.015670   31.095238   \n429759   0.115882  0.152323  0.008917  0.011721  0.028019   25.063830   \n4156     0.102335  0.136832  0.011378  0.015214  0.024893   22.983051   \n384353   0.092425  0.140796  0.009963  0.015177  0.025054   21.250000   \n2948423  0.085382  0.160420  0.009637  0.018107  0.019580   32.076923   \n289739   0.094039  0.113379  0.007408  0.008932  0.023780   22.837209   \n3824542  0.178554  0.263851  0.008153  0.012047  0.022823  -29.400000   \n1189993  0.153063  0.232305  0.023306  0.035372  0.019617   35.135542   \n3217957  0.095562  0.126489  0.026261  0.034759  0.021369    9.106383   \n190451   0.103257  0.158285  0.006139  0.009411  0.020435   26.453333   \n1929247  0.089601  0.215439  0.006852  0.016476  0.034269   15.038356   \n899642   0.101737  0.161720  0.007851  0.012479  0.022150   31.322581   \n128258   0.092568  0.245541  0.004122  0.010934  0.033177    3.076923   \n255547   0.116696  0.169720  0.009417  0.013696  0.018711   41.866667   \n790703   0.093573  0.111329  0.008866  0.010548  0.021677   17.333333   \n702836   0.104390  0.164841  0.007787  0.012297  0.022364    8.645161   \n3351598  0.012729  0.016995  0.012958  0.017301  0.053146 -130.133333   \n954671   0.083234  0.105325  0.010639  0.013462  0.030926   10.825000   \n489155   0.080357  0.121918  0.003092  0.004691  0.021610   10.750000   \n183846   0.169514  0.279477  0.007186  0.011847  0.023957   20.589041   \n\n              ROA       ROE      TDTL      TDTA  ...   EQTL-4Q  LLRTA-4Q  \\\n487142   0.007850  0.057949  1.390573  0.857393  ...  0.219331  0.005631   \n3593307  0.003842  0.041644  0.980606  0.869490  ...  0.115903  0.025644   \n3134250  0.001654  0.020079  0.934364  0.765143  ...  0.092728  0.007699   \n537355   0.004365  0.049776  2.102950  0.888647  ...  0.193911  0.007593   \n307062   0.002068  0.016177  1.270097  0.868712  ...  0.162100  0.032975   \n3482894 -0.002327 -0.018148  1.078908  0.790864  ...  0.178732  0.014612   \n2078290  0.004292  0.031960  1.384858  0.817240  ...  0.230841  0.016710   \n518354   0.004119  0.027696  1.135113  0.822715  ...  0.208783  0.013134   \n904667   0.006657  0.087372  2.125357  0.918890  ...  0.177338  0.004462   \n613334   0.007488  0.069151  1.498138  0.888288  ...  0.190050  0.005469   \n391418   0.002075  0.011569  1.759689  0.793029  ...  0.379252  0.007919   \n349129   0.008588  0.052977  1.835794  0.833799  ...  0.334473  0.007731   \n2140348  0.002048  0.027462  1.043680  0.791079  ...  0.091393  0.016142   \n117458   0.006699  0.058058  1.177048  0.806153  ...  0.150928  0.007527   \n834979  -0.016508 -0.297695  0.929028  0.769886  ...  0.088498  0.019076   \n340144   0.006562  0.065896  1.017093  0.755581  ...  0.126852  0.012547   \n2068107 -0.013771 -0.098949  1.307066  0.851654  ...  0.190256  0.023585   \n158947   0.006771  0.053584  1.228648  0.817956  ...  0.182142  0.008376   \n122153   0.002252  0.017476  2.777437  0.863188  ...  0.383238  0.004826   \n750154   0.012172  0.156391  1.286908  0.918917  ...  0.107165  0.007292   \n722227  -0.004876 -0.111969  1.352172  0.953344  ...  0.090439  0.028958   \n6141     0.005434  0.060054  2.029462  0.906449  ...  0.211396  0.008531   \n444677   0.003695  0.034634  1.165282  0.889351  ...  0.134106  0.000636   \n2936840 -0.002172 -0.023161  1.337444  0.903577  ...  0.126431  0.010341   \n937759   0.010468  0.082824  1.492913  0.865644  ...  0.224580  0.005402   \n803658   0.010374  0.067245  1.407891  0.834373  ...  0.276373  0.007850   \n200435   0.011942  0.127185  1.202771  0.878662  ...  0.103804  0.014219   \n719030   0.007862  0.072571  1.476289  0.882989  ...  0.168435  0.005419   \n3548763  0.004286  0.030748  1.306679  0.838687  ...  0.199519  0.008692   \n3067929 -0.014474 -0.129903  0.951739  0.717682  ...  0.188183  0.041104   \n...           ...       ...       ...       ...  ...       ...       ...   \n999935   0.000179  0.001648  1.319114  0.867532  ...  0.157289  0.018666   \n412845   0.007271  0.078873  1.702123  0.881238  ...  0.172329  0.005626   \n3623969  0.010698  0.078783  1.525457  0.846780  ...  0.255154  0.016017   \n794345   0.004215  0.028692  2.310042  0.810792  ...  0.360378  0.010324   \n895831  -0.027405 -0.695180  1.178948  0.908321  ...  0.078574  0.030638   \n173342   0.012862  0.115334  2.249141  0.863318  ...  0.274619  0.008551   \n805876   0.004499  0.033926  1.007645  0.774532  ...  0.184191  0.005097   \n1225798  0.011868  0.078575  0.669550  0.626528  ...  0.153664  0.015506   \n477303   0.004153  0.047083  0.894013  0.742597  ...  0.106789  0.008504   \n1669     0.010788  0.059227  2.410124  0.814387  ...  0.518946  0.007971   \n3602889  0.004323  0.041934  1.241417  0.886397  ...  0.134116  0.011124   \n429759   0.008350  0.072053  1.086191  0.826336  ...  0.153566  0.008760   \n4156     0.007244  0.070784  0.980858  0.733576  ...  0.126550  0.012397   \n384353   0.008384  0.090715  1.378512  0.904912  ...  0.137619  0.011073   \n2948423  0.005932  0.069471  1.627233  0.866078  ...  0.186740  0.008285   \n289739   0.008039  0.085481  1.052130  0.872661  ...  0.108986  0.006505   \n3824542 -0.004939 -0.027659  1.210300  0.819038  ...  0.469209  0.007213   \n1189993  0.005480  0.035799  1.151275  0.758562  ...  0.211251  0.023313   \n3217957  0.002352  0.024616  1.129151  0.853076  ...  0.124632  0.027548   \n190451   0.008337  0.080742  1.352792  0.882493  ...  0.150606  0.005720   \n1929247  0.004434  0.049491  1.934952  0.804748  ...  0.228262  0.008210   \n899642   0.008643  0.084952  1.365328  0.858913  ...  0.145646  0.005596   \n128258   0.001601  0.017294  2.400955  0.905151  ...  0.287490  0.004104   \n255547   0.008646  0.074092  1.202279  0.826663  ...  0.174019  0.009023   \n790703   0.003596  0.038429  0.739489  0.621548  ...  0.104297  0.006499   \n702836   0.002817  0.026981  1.409556  0.892636  ...  0.158126  0.005857   \n3351598 -0.040667 -3.194763  1.307669  0.979417  ...  0.056874  0.025144   \n954671   0.004563  0.054824  0.997973  0.788658  ...  0.098272  0.010608   \n489155   0.002829  0.035203  1.383521  0.911881  ...  0.131835  0.003241   \n183846   0.006742  0.039770  1.302602  0.790081  ...  0.271255  0.006824   \n\n         LLRGL-4Q  OEXTA-4Q   INCEMP-4Q    ROA-4Q    ROE-4Q   TDTL-4Q  \\\n487142   0.009076  0.022150   42.636364  0.010042  0.073789  1.382277   \n3593307  0.028888  0.014242   50.800000  0.003302  0.032095  0.966623   \n3134250  0.008618  0.037980    0.238095  0.000094  0.001140  0.786979   \n537355   0.015515  0.036625   15.731707  0.007077  0.074575  1.801381   \n307062   0.045195  0.025085  -50.659341 -0.011837 -0.100087  1.206137   \n3482894  0.018893  0.038533  -87.655172 -0.019009 -0.137517  0.997138   \n2078290  0.026230  0.028958   -4.026685 -0.000896 -0.006092  1.258679   \n518354   0.019214  0.055568    7.636364  0.004041  0.028316  1.232713   \n904667   0.010503  0.037885   28.938776  0.011279  0.149704  2.159065   \n613334   0.009503  0.023646   19.800000  0.005308  0.048529  1.541969   \n391418   0.017168  0.037993   11.921053  0.004617  0.026392  1.745434   \n349129   0.015760  0.030306   27.125000  0.011025  0.067197  1.696427   \n2140348  0.021233  0.026165  -57.617647 -0.011879 -0.170972  0.929250   \n117458   0.010229  0.024429   22.619718  0.006183  0.055673  1.096756   \n834979   0.023404  0.030443  -47.331839 -0.012507 -0.173391  0.966745   \n340144   0.017290  0.023852   28.153846  0.006034  0.065544  1.044889   \n2068107  0.035150  0.037553 -114.743590 -0.036445 -0.285486  1.224690   \n158947   0.011702  0.027492   38.741935  0.007820  0.059978  1.083530   \n122153   0.019244  0.026134   19.687500  0.004781  0.049739  3.597458   \n750154   0.010174  0.025798   43.916667  0.012843  0.167217  1.280753   \n722227   0.044322  0.069322  -32.882353 -0.019862 -0.336140  1.435610   \n6141     0.018994  0.024404   27.000000  0.006195  0.065246  2.008684   \n444677   0.000826  0.024176   23.833333  0.003956  0.038286  1.158019   \n2936840  0.016362  0.033372   -4.678571 -0.001485 -0.018589  1.447504   \n937759   0.011566  0.023198   50.687500  0.012169  0.116015  1.907283   \n803658   0.017293  0.022407   51.583333  0.011069  0.088227  1.577838   \n200435   0.019307  0.023984   50.774834  0.013238  0.173164  1.174067   \n719030   0.008481  0.028375   38.181818  0.008399  0.078038  1.388696   \n3548763  0.013321  0.036415  -31.360000 -0.007691 -0.059081  1.299218   \n3067929  0.060884  0.029688 -247.921053 -0.045397 -0.357330  1.180967   \n...           ...       ...         ...       ...       ...       ...   \n999935   0.026934  0.037992    3.110000  0.001173  0.010763  1.251351   \n412845   0.010764  0.022869   32.287671  0.008600  0.095471  1.630576   \n3623969  0.029560  0.031714   48.750000  0.013861  0.100257  1.564445   \n794345   0.027766  0.011665   53.571429  0.004372  0.032629  2.173636   \n895831   0.036518  0.036625 -208.646341 -0.039427 -0.598091  1.053491   \n173342   0.022750  0.023060   73.000000  0.014792  0.143306  2.324792   \n805876   0.007450  0.026974   19.000000  0.005991  0.047537  1.094636   \n1225798  0.016520  0.033370   35.000000  0.006392  0.044314  0.655194   \n477303   0.010837  0.030248   14.644068  0.004649  0.055475  0.896841   \n1669     0.024153  0.020969   52.152542  0.012546  0.073255  2.496510   \n3602889  0.014795  0.020256    8.631579  0.001215  0.012045  1.177830   \n429759   0.012703  0.034971   13.804348  0.004353  0.041100  1.268845   \n4156     0.017842  0.034520    5.176471  0.001719  0.019546  1.127239   \n384353   0.016314  0.034207   16.500000  0.007236  0.077465  1.332095   \n2948423  0.015080  0.027507   62.333333  0.012470  0.121537  1.622726   \n289739   0.007778  0.032341   29.976190  0.010905  0.119643  1.011890   \n3824542  0.011770  0.044154 -110.857143 -0.027643 -0.096129  1.159813   \n1189993  0.034304  0.028324  -21.289003 -0.003745 -0.026088  1.130386   \n3217957  0.034742  0.029404  -87.378378 -0.018810 -0.190333  1.069521   \n190451   0.008786  0.026679   28.000000  0.009199  0.093817  1.379471   \n1929247  0.019564  0.039259   21.180180  0.005338  0.055724  1.581081   \n899642   0.008629  0.030212   12.475410  0.003442  0.036450  1.305955   \n128258   0.013971  0.042059   -5.307692 -0.002598 -0.030762  3.109203   \n255547   0.013428  0.024831   48.000000  0.010072  0.086135  1.219444   \n790703   0.007791  0.029339   30.310345  0.006230  0.071603  0.754687   \n702836   0.008697  0.031258   13.612903  0.004603  0.043224  1.321418   \n3351598  0.033586  0.069404 -179.687500 -0.050375 -1.183128  1.263516   \n954671   0.013080  0.041345    7.936709  0.003326  0.041728  0.992302   \n489155   0.005535  0.029038    8.250000  0.002161  0.027990  1.568042   \n183846   0.011426  0.030643   26.473684  0.009104  0.056201  1.334068   \n\n          TDTA-4Q   TATA-4Q  \n487142   0.857678  0.258447  \n3593307  0.858089  0.000000  \n3134250  0.703121  0.039611  \n537355   0.881587  0.309527  \n307062   0.880024  0.095439  \n3482894  0.771170  0.131618  \n2078290  0.801821  0.193432  \n518354   0.842655  0.000120  \n904667   0.917247  0.000000  \n613334   0.887480  0.141689  \n391418   0.805051  0.455112  \n349129   0.832146  0.404583  \n2140348  0.706439  0.064555  \n117458   0.807066  0.083997  \n834979   0.787936  0.066190  \n340144   0.758275  0.175070  \n2068107  0.821750  0.000000  \n158947   0.775581  0.145845  \n122153   0.902204  0.216855  \n750154   0.917916  0.150121  \n722227   0.937962  0.191870  \n6141     0.902176  0.000000  \n444677   0.892122  0.000000  \n2936840  0.914865  0.210332  \n937759   0.890823  0.293507  \n803658   0.716278  0.385884  \n200435   0.864649  0.113722  \n719030   0.887318  0.239482  \n3548763  0.847699  0.264705  \n3067929  0.797291  0.079523  \n...           ...       ...  \n999935   0.867201  0.140521  \n412845   0.852289  0.393662  \n3623969  0.847684  0.000000  \n794345   0.808223  0.557049  \n895831   0.883854  0.073596  \n173342   0.873826  0.455495  \n805876   0.748956  0.186706  \n1225798  0.615012  0.000100  \n477303   0.703805  0.088539  \n1669     0.823894  0.147797  \n3602889  0.885625  0.118922  \n429759   0.875009  0.157995  \n4156     0.783229  0.176553  \n384353   0.904177  0.000000  \n2948423  0.891573  0.340877  \n289739   0.846227  0.054930  \n3824542  0.710802  0.158643  \n1189993  0.768207  0.231102  \n3217957  0.848057  0.125041  \n190451   0.898100  0.281662  \n1929247  0.663482  0.352342  \n899642   0.846825  0.240930  \n128258   0.913397  0.110438  \n255547   0.819428  0.212383  \n790703   0.629572  0.096865  \n702836   0.889823  0.063683  \n3351598  0.945910  0.068054  \n954671   0.804749  0.070803  \n489155   0.918156  0.000000  \n183846   0.796696  0.289001  \n\n[5513 rows x 55 columns]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bff0a5bcbe8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#(https://medium.com/analytics-vidhya/introduction-to-neural-networks-for-finance-6abd5675e497)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2543\u001b[0m             not tensor_util.is_tensor(x_input)):\n\u001b[1;32m   2544\u001b[0m           raise ValueError('Please provide as model inputs either a single '\n\u001b[0;32m-> 2545\u001b[0;31m                            'array or a list of arrays. You passed: x=' + str(x))\n\u001b[0m\u001b[1;32m   2546\u001b[0m         \u001b[0mall_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Please provide as model inputs either a single array or a list of arrays. You passed: x=             EQTA      EQTL     LLRTA     LLRGL     OEXTA      INCEMP  \\\n487142   0.135469  0.219712  0.004861  0.007883  0.015192   35.090909   \n3593307  0.092253  0.104042  0.036328  0.040970  0.012562   25.666667   \n3134250  0.082356  0.100571  0.008513  0.010396  0.036723    4.153846   \n537355   0.087692  0.207520  0.005452  0.012901  0.024309   10.622222   \n307062   0.127852  0.186926  0.027541  0.040266  0.021548    8.466667   \n3482894  0.128233  0.174938  0.013176  0.017974  0.026977  -11.821429   \n2078290  0.134291  0.227563  0.014798  0.025075  0.025034   16.555455   \n518354   0.148720  0.205192  0.012503  0.017251  0.052498    7.347826   \n904667   0.076193  0.176231  0.004657  0.010771  0.027759   17.541667   \n613334   0.108285  0.182628  0.005635  0.009504  0.016728   26.818182   \n391418   0.179367  0.398005  0.013135  0.029146  0.029504    5.166667   \n349129   0.162108  0.356917  0.007100  0.015632  0.019909   25.704545   \n2140348  0.074561  0.098370  0.013868  0.018296  0.019760   10.393939   \n117458   0.115381  0.168465  0.011076  0.016171  0.016834   24.054054   \n834979   0.055454  0.066917  0.019895  0.024007  0.023497  -61.342105   \n340144   0.099588  0.134056  0.013343  0.017962  0.019614   31.480000   \n2068107  0.139172  0.213593  0.025532  0.039184  0.034852  -39.657895   \n158947   0.126356  0.189798  0.008001  0.012019  0.018810   36.885246   \n122153   0.128872  0.414666  0.006031  0.019407  0.024545    6.941176   \n750154   0.077829  0.108996  0.008580  0.012016  0.018551   43.883333   \n722227   0.043545  0.061762  0.020988  0.029768  0.042536   -9.666667   \n6141     0.090484  0.202586  0.006494  0.014540  0.016302   26.090909   \n444677   0.106678  0.139776  0.001241  0.001626  0.019108   22.333333   \n2936840  0.093773  0.138799  0.017552  0.025981  0.030092   -6.625000   \n937759   0.126390  0.217976  0.006359  0.010967  0.020328   36.515152   \n803658   0.154279  0.260325  0.008834  0.014907  0.018338   42.666667   \n200435   0.093891  0.128525  0.012523  0.017142  0.019415   44.386667   \n719030   0.108328  0.181117  0.005800  0.009698  0.018258   43.100000   \n3548763  0.139389  0.217168  0.009648  0.015032  0.023446   17.000000   \n3067929  0.111422  0.147760  0.049365  0.065465  0.021013  -79.921053   \n...           ...       ...       ...       ...       ...         ...   \n999935   0.108819  0.165464  0.019108  0.029055  0.026945    0.466019   \n412845   0.092181  0.178049  0.005764  0.011134  0.015931   28.635135   \n3623969  0.135794  0.244630  0.015847  0.028548  0.026008   34.629630   \n794345   0.146918  0.418587  0.005396  0.015374  0.008319   51.000000   \n895831   0.039422  0.051168  0.037432  0.048585  0.028732 -130.747253   \n173342   0.111518  0.290531  0.009037  0.023542  0.016247   60.312500   \n805876   0.132621  0.172537  0.005226  0.006799  0.021379   14.341463   \n1225798  0.151043  0.161415  0.016558  0.017695  0.018774   67.181818   \n477303   0.088201  0.106185  0.006826  0.008218  0.023050   13.271930   \n1669     0.182143  0.539039  0.008249  0.024414  0.014856   47.122807   \n3602889  0.103090  0.144379  0.011877  0.016633  0.015670   31.095238   \n429759   0.115882  0.152323  0.008917  0.011721  0.028019   25.063830   \n4156     0.102335  0.136832  0.011378  0.015214  0.024893   22.983051   \n384353   0.092425  0.140796  0.009963  0.015177  0.025054   21.250000   \n2948423  0.085382  0.160420  0.009637  0.018107  0.019580   32.076923   \n289739   0.094039  0.113379  0.007408  0.008932  0.023780   22.837209   \n3824542  0.178554  0.263851  0.008153  0.012047  0.022823  -29.400000   \n1189993  0.153063  0.232305  0.023306  0.035372  0.019617   35.135542   \n3217957  0.095562  0.126489  0.026261  0.034759  0.021369    9.106383   \n190451   0.103257  0.158285  0.006139  0.009411  0.020435   26.453333   \n1929247  0.089601  0.215439  0.006852  0.016476  0.034269   15.038356   \n899642   0.101737  0.161720  0.007851  0.012479  0.022150   31.322581   \n128258   0.092568  0.245541  0.004122  0.010934  0.033177    3.076923   \n255547   0.116696  0.169720  0.009417  0.013696  0.018711   41.866667   \n790703   0.093573  0.111329  0.008866  0.010548  0.021677   17.333333   \n702836   0.104390  0.164841  0.007787  0.012297  0.022364    8.645161   \n3351598  0.012729  0.016995  0.012958  0.017301  0.053146 -130.133333   \n954671   0.083234  0.105325  0.010639  0.013462  0.030926   10.825000   \n489155   0.080357  0.121918  0.003092  0.004691  0.021610   10.750000   \n183846   0.169514  0.279477  0.007186  0.011847  0.023957   20.589041   \n\n              ROA       ROE      TDTL      TDTA  ...   EQTL-4Q  LLRTA-4Q  \\\n487142   0.007850  0.057949  1.390573  0.857393  ...  0.219331  0.005631   \n3593307  0.003842  0.041644  0.980606  0.869490  ...  0.115903  0.025644   \n3134250  0.001654  0.020079  0.934364  0.765143  ...  0.092728  0.007699   \n537355   0.004365  0.049776  2.102950  0.888647  ...  0.193911  0.007593   \n307062   0.002068  0.016177  1.270097  0.868712  ...  0.162100  0.032975   \n3482894 -0.002327 -0.018148  1.078908  0.790864  ...  0.178732  0.014612   \n2078290  0.004292  0.031960  1.384858  0.817240  ...  0.230841  0.016710   \n518354   0.004119  0.027696  1.135113  0.822715  ...  0.208783  0.013134   \n904667   0.006657  0.087372  2.125357  0.918890  ...  0.177338  0.004462   \n613334   0.007488  0.069151  1.498138  0.888288  ...  0.190050  0.005469   \n391418   0.002075  0.011569  1.759689  0.793029  ...  0.379252  0.007919   \n349129   0.008588  0.052977  1.835794  0.833799  ...  0.334473  0.007731   \n2140348  0.002048  0.027462  1.043680  0.791079  ...  0.091393  0.016142   \n117458   0.006699  0.058058  1.177048  0.806153  ...  0.150928  0.007527   \n834979  -0.016508 -0.297695  0.929028  0.769886  ...  0.088498  0.019076   \n340144   0.006562  0.065896  1.017093  0.755581  ...  0.126852  0.012547   \n2068107 -0.013771 -0.098949  1.307066  0.851654  ...  0.190256  0.023585   \n158947   0.006771  0.053584  1.228648  0.817956  ...  0.182142  0.008376   \n122153   0.002252  0.017476  2.777437  0.863188  ...  0.383238  0.004826   \n750154   0.012172  0.156391  1.286908  0.918917  ...  0.107165  0.007292   \n722227  -0.004876 -0.111969  1.352172  0.953344  ...  0.090439  0.028958   \n6141     0.005434  0.060054  2.029462  0.906449  ...  0.211396  0.008531   \n444677   0.003695  0.034634  1.165282  0.889351  ...  0.134106  0.000636   \n2936840 -0.002172 -0.023161  1.337444  0.903577  ...  0.126431  0.010341   \n937759   0.010468  0.082824  1.492913  0.865644  ...  0.224580  0.005402   \n803658   0.010374  0.067245  1.407891  0.834373  ...  0.276373  0.007850   \n200435   0.011942  0.127185  1.202771  0.878662  ...  0.103804  0.014219   \n719030   0.007862  0.072571  1.476289  0.882989  ...  0.168435  0.005419   \n3548763  0.004286  0.030748  1.306679  0.838687  ...  0.199519  0.008692   \n3067929 -0.014474 -0.129903  0.951739  0.717682  ...  0.188183  0.041104   \n...           ...       ...       ...       ...  ...       ...       ...   \n999935   0.000179  0.001648  1.319114  0.867532  ...  0.157289  0.018666   \n412845   0.007271  0.078873  1.702123  0.881238  ...  0.172329  0.005626   \n3623969  0.010698  0.078783  1.525457  0.846780  ...  0.255154  0.016017   \n794345   0.004215  0.028692  2.310042  0.810792  ...  0.360378  0.010324   \n895831  -0.027405 -0.695180  1.178948  0.908321  ...  0.078574  0.030638   \n173342   0.012862  0.115334  2.249141  0.863318  ...  0.274619  0.008551   \n805876   0.004499  0.033926  1.007645  0.774532  ...  0.184191  0.005097   \n1225798  0.011868  0.078575  0.669550  0.626528  ...  0.153664  0.015506   \n477303   0.004153  0.047083  0.894013  0.742597  ...  0.106789  0.008504   \n1669     0.010788  0.059227  2.410124  0.814387  ...  0.518946  0.007971   \n3602889  0.004323  0.041934  1.241417  0.886397  ...  0.134116  0.011124   \n429759   0.008350  0.072053  1.086191  0.826336  ...  0.153566  0.008760   \n4156     0.007244  0.070784  0.980858  0.733576  ...  0.126550  0.012397   \n384353   0.008384  0.090715  1.378512  0.904912  ...  0.137619  0.011073   \n2948423  0.005932  0.069471  1.627233  0.866078  ...  0.186740  0.008285   \n289739   0.008039  0.085481  1.052130  0.872661  ...  0.108986  0.006505   \n3824542 -0.004939 -0.027659  1.210300  0.819038  ...  0.469209  0.007213   \n1189993  0.005480  0.035799  1.151275  0.758562  ...  0.211251  0.023313   \n3217957  0.002352  0.024616  1.129151  0.853076  ...  0.124632  0.027548   \n190451   0.008337  0.080742  1.352792  0.882493  ...  0.150606  0.005720   \n1929247  0.004434  0.049491  1.934952  0.804748  ...  0.228262  0.008210   \n899642   0.008643  0.084952  1.365328  0.858913  ...  0.145646  0.005596   \n128258   0.001601  0.017294  2.400955  0.905151  ...  0.287490  0.004104   \n255547   0.008646  0.074092  1.202279  0.826663  ...  0.174019  0.009023   \n790703   0.003596  0.038429  0.739489  0.621548  ...  0.104297  0.006499   \n702836   0.002817  0.026981  1.409556  0.892636  ...  0.158126  0.005857   \n3351598 -0.040667 -3.194763  1.307669  0.979417  ...  0.056874  0.025144   \n954671   0.004563  0.054824  0.997973  0.788658  ...  0.098272  0.010608   \n489155   0.002829  0.035203  1.383521  0.911881  ...  0.131835  0.003241   \n183846   0.006742  0.039770  1.302602  0.790081  ...  0.271255  0.006824   \n\n         LLRGL-4Q  OEXTA-4Q   INCEMP-4Q    ROA-4Q    ROE-4Q   TDTL-4Q  \\\n487142   0.009076  0.022150   42.636364  0.010042  0.073789  1.382277   \n3593307  0.028888  0.014242   50.800000  0.003302  0.032095  0.966623   \n3134250  0.008618  0.037980    0.238095  0.000094  0.001140  0.786979   \n537355   0.015515  0.036625   15.731707  0.007077  0.074575  1.801381   \n307062   0.045195  0.025085  -50.659341 -0.011837 -0.100087  1.206137   \n3482894  0.018893  0.038533  -87.655172 -0.019009 -0.137517  0.997138   \n2078290  0.026230  0.028958   -4.026685 -0.000896 -0.006092  1.258679   \n518354   0.019214  0.055568    7.636364  0.004041  0.028316  1.232713   \n904667   0.010503  0.037885   28.938776  0.011279  0.149704  2.159065   \n613334   0.009503  0.023646   19.800000  0.005308  0.048529  1.541969   \n391418   0.017168  0.037993   11.921053  0.004617  0.026392  1.745434   \n349129   0.015760  0.030306   27.125000  0.011025  0.067197  1.696427   \n2140348  0.021233  0.026165  -57.617647 -0.011879 -0.170972  0.929250   \n117458   0.010229  0.024429   22.619718  0.006183  0.055673  1.096756   \n834979   0.023404  0.030443  -47.331839 -0.012507 -0.173391  0.966745   \n340144   0.017290  0.023852   28.153846  0.006034  0.065544  1.044889   \n2068107  0.035150  0.037553 -114.743590 -0.036445 -0.285486  1.224690   \n158947   0.011702  0.027492   38.741935  0.007820  0.059978  1.083530   \n122153   0.019244  0.026134   19.687500  0.004781  0.049739  3.597458   \n750154   0.010174  0.025798   43.916667  0.012843  0.167217  1.280753   \n722227   0.044322  0.069322  -32.882353 -0.019862 -0.336140  1.435610   \n6141     0.018994  0.024404   27.000000  0.006195  0.065246  2.008684   \n444677   0.000826  0.024176   23.833333  0.003956  0.038286  1.158019   \n2936840  0.016362  0.033372   -4.678571 -0.001485 -0.018589  1.447504   \n937759   0.011566  0.023198   50.687500  0.012169  0.116015  1.907283   \n803658   0.017293  0.022407   51.583333  0.011069  0.088227  1.577838   \n200435   0.019307  0.023984   50.774834  0.013238  0.173164  1.174067   \n719030   0.008481  0.028375   38.181818  0.008399  0.078038  1.388696   \n3548763  0.013321  0.036415  -31.360000 -0.007691 -0.059081  1.299218   \n3067929  0.060884  0.029688 -247.921053 -0.045397 -0.357330  1.180967   \n...           ...       ...         ...       ...       ...       ...   \n999935   0.026934  0.037992    3.110000  0.001173  0.010763  1.251351   \n412845   0.010764  0.022869   32.287671  0.008600  0.095471  1.630576   \n3623969  0.029560  0.031714   48.750000  0.013861  0.100257  1.564445   \n794345   0.027766  0.011665   53.571429  0.004372  0.032629  2.173636   \n895831   0.036518  0.036625 -208.646341 -0.039427 -0.598091  1.053491   \n173342   0.022750  0.023060   73.000000  0.014792  0.143306  2.324792   \n805876   0.007450  0.026974   19.000000  0.005991  0.047537  1.094636   \n1225798  0.016520  0.033370   35.000000  0.006392  0.044314  0.655194   \n477303   0.010837  0.030248   14.644068  0.004649  0.055475  0.896841   \n1669     0.024153  0.020969   52.152542  0.012546  0.073255  2.496510   \n3602889  0.014795  0.020256    8.631579  0.001215  0.012045  1.177830   \n429759   0.012703  0.034971   13.804348  0.004353  0.041100  1.268845   \n4156     0.017842  0.034520    5.176471  0.001719  0.019546  1.127239   \n384353   0.016314  0.034207   16.500000  0.007236  0.077465  1.332095   \n2948423  0.015080  0.027507   62.333333  0.012470  0.121537  1.622726   \n289739   0.007778  0.032341   29.976190  0.010905  0.119643  1.011890   \n3824542  0.011770  0.044154 -110.857143 -0.027643 -0.096129  1.159813   \n1189993  0.034304  0.028324  -21.289003 -0.003745 -0.026088  1.130386   \n3217957  0.034742  0.029404  -87.378378 -0.018810 -0.190333  1.069521   \n190451   0.008786  0.026679   28.000000  0.009199  0.093817  1.379471   \n1929247  0.019564  0.039259   21.180180  0.005338  0.055724  1.581081   \n899642   0.008629  0.030212   12.475410  0.003442  0.036450  1.305955   \n128258   0.013971  0.042059   -5.307692 -0.002598 -0.030762  3.109203   \n255547   0.013428  0.024831   48.000000  0.010072  0.086135  1.219444   \n790703   0.007791  0.029339   30.310345  0.006230  0.071603  0.754687   \n702836   0.008697  0.031258   13.612903  0.004603  0.043224  1.321418   \n3351598  0.033586  0.069404 -179.687500 -0.050375 -1.183128  1.263516   \n954671   0.013080  0.041345    7.936709  0.003326  0.041728  0.992302   \n489155   0.005535  0.029038    8.250000  0.002161  0.027990  1.568042   \n183846   0.011426  0.030643   26.473684  0.009104  0.056201  1.334068   \n\n          TDTA-4Q   TATA-4Q  \n487142   0.857678  0.258447  \n3593307  0.858089  0.000000  \n3134250  0.703121  0.039611  \n537355   0.881587  0.309527  \n307062   0.880024  0.095439  \n3482894  0.771170  0.131618  \n2078290  0.801821  0.193432  \n518354   0.842655  0.000120  \n904667   0.917247  0.000000  \n613334   0.887480  0.141689  \n391418   0.805051  0.455112  \n349129   0.832146  0.404583  \n2140348  0.706439  0.064555  \n117458   0.807066  0.083997  \n834979   0.787936  0.066190  \n340144   0.758275  0.175070  \n2068107  0.821750  0.000000  \n158947   0.775581  0.145845  \n122153   0.902204  0.216855  \n750154   0.917916  0.150121  \n722227   0.937962  0.191870  \n6141     0.902176  0.000000  \n444677   0.892122  0.000000  \n2936840  0.914865  0.210332  \n937759   0.890823  0.293507  \n803658   0.716278  0.385884  \n200435   0.864649  0.113722  \n719030   0.887318  0.239482  \n3548763  0.847699  0.264705  \n3067929  0.797291  0.079523  \n...           ...       ...  \n999935   0.867201  0.140521  \n412845   0.852289  0.393662  \n3623969  0.847684  0.000000  \n794345   0.808223  0.557049  \n895831   0.883854  0.073596  \n173342   0.873826  0.455495  \n805876   0.748956  0.186706  \n1225798  0.615012  0.000100  \n477303   0.703805  0.088539  \n1669     0.823894  0.147797  \n3602889  0.885625  0.118922  \n429759   0.875009  0.157995  \n4156     0.783229  0.176553  \n384353   0.904177  0.000000  \n2948423  0.891573  0.340877  \n289739   0.846227  0.054930  \n3824542  0.710802  0.158643  \n1189993  0.768207  0.231102  \n3217957  0.848057  0.125041  \n190451   0.898100  0.281662  \n1929247  0.663482  0.352342  \n899642   0.846825  0.240930  \n128258   0.913397  0.110438  \n255547   0.819428  0.212383  \n790703   0.629572  0.096865  \n702836   0.889823  0.063683  \n3351598  0.945910  0.068054  \n954671   0.804749  0.070803  \n489155   0.918156  0.000000  \n183846   0.796696  0.289001  \n\n[5513 rows x 55 columns]"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) \n",
    "# source: https://medium.com/engineer-quant/multilayer-perceptron-4453615c4337\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Adam optimizer is gaining popularity in the machine learning community because it is \n",
    "#a more efficient algorithm to optimize compared to traditional stochastic gradient descent. \n",
    "#The advantages are best understood by looking at  the advantages of two other extensions of\n",
    "#stochastic gradient descent \n",
    "#(https://medium.com/analytics-vidhya/introduction-to-neural-networks-for-finance-6abd5675e497)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Long Short Term Model (LSTM)\n",
    "# source: https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up the data into training sets\n",
    "def gen_train(self, seq_len):\n",
    "    \"\"\"\n",
    "    Generates training data\n",
    "    :param seq_len: length of window\n",
    "    :return: X_train and Y_train\n",
    "    \"\"\"\n",
    "    for i in range((len(self.stock_train)//seq_len)*seq_len - seq_len - 1):\n",
    "        x = np.array(self.stock_train.iloc[i: i + seq_len, 1])\n",
    "        y = np.array([self.stock_train.iloc[i + seq_len + 1, 1]], np.float64)\n",
    "        self.input_train.append(x)\n",
    "        self.output_train.append(y)\n",
    "    self.X_train = np.array(self.input_train)\n",
    "    self.Y_train = np.array(self.output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_input to have 3 dimensions, but got array with shape (5513, 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2dcd096a5d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    374\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_input to have 3 dimensions, but got array with shape (5513, 55)"
     ]
    }
   ],
   "source": [
    "# Long Short Term Model (LSTM)\n",
    "# source: https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(20, input_shape=(10, 1), return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(20))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "model.fit(X_train, y_train, epochs=20)\n",
    "\n",
    "model.evaluate(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on Backtesting - check how to do it?\n",
    "\n",
    "#Example below. For full blown backtest systems, you will need to consider factors such as \n",
    "#survivorship bias, look ahead bias, market regime change and transaction costs.\n",
    "\n",
    "#Now that we have fitted our models using our training data and evaluated it using our test data, \n",
    "#we can take the assessment a step further by backtesting the model on new data. Backtesting is \n",
    "#essentially running your strategy (or i our case, the prediction algorithm) over data from a period \n",
    "#of time to see the profit and loss, or accuracy of the algorithm. This is done simply by\n",
    "\n",
    "def back_test(strategy, seq_len, ticker, start_date, end_date, dim):\n",
    "    \"\"\"\n",
    "    A simple back test for a given date period\n",
    "    :param strategy: the chosen strategy. Note to have already formed the model, and fitted with training data.\n",
    "    :param seq_len: length of the days used for prediction\n",
    "    :param ticker: company ticker\n",
    "    :param start_date: starting date\n",
    "    :type start_date: \"YYYY-mm-dd\"\n",
    "    :param end_date: ending date\n",
    "    :type end_date: \"YYYY-mm-dd\"\n",
    "    :param dim: dimension required for strategy: 3dim for LSTM and 2dim for MLP\n",
    "    :type dim: tuple\n",
    "    :return: Percentage errors array that gives the errors for every test in the given date range\n",
    "    \"\"\"\n",
    "    data = pdr.get_data_yahoo(ticker, start_date, end_date)\n",
    "    stock_data = data[\"Adj Close\"]\n",
    "    errors = []\n",
    "    for i in range((len(stock_data)//10)*10 - seq_len - 1):\n",
    "        x = np.array(stock_data.iloc[i: i + seq_len, 1]).reshape(dim) / 200\n",
    "        y = np.array(stock_data.iloc[i + seq_len + 1, 1]) / 200\n",
    "        predict = strategy.predict(x)\n",
    "        while predict == 0:\n",
    "            predict = strategy.predict(x)\n",
    "        error = (predict - y) / 100\n",
    "        errors.append(error)\n",
    "        total_error = np.array(errors)\n",
    "    print(f\"Average error = {total_error.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
